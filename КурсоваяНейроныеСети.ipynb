{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP89sGKRHfd/lNUWbmGTdqW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniilsol200/KURSOVAI_kovalevski_Kurs3/blob/main/%D0%9A%D1%83%D1%80%D1%81%D0%BE%D0%B2%D0%B0%D1%8F%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D1%8B%D0%B5%D0%A1%D0%B5%D1%82%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "g4CXP0oguzIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703e1041-1656-4c68-bc1e-77390977a561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len text=19058, vocab=66, train=17151, test=1906\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42); random.seed(42)\n",
        "\n",
        "# =========================\n",
        "# 1) Подготовка данных\n",
        "# =========================\n",
        "text = \"\"\"\n",
        "Нейронные — это мощный инструмент, который способен находить закономерности в данных.\n",
        "Они применяются в обработке языка, изображений, в прогнозировании и многом другом.\n",
        "В этой небольшой демонстрации мы обучаем простые рекуррентные сети для предсказания следующего символа.\n",
        "Искусственный интеллект, нейронные сети, машинное обучение — что на самом деле означают все эти нынче популярные понятия?\n",
        "Для большинства непосвященных людей, коим являюсь и я сам, они всегда казались чем-то фантастическим.\n",
        "Впервые понятие искусственных нейронных сетей (ИНС) возникло при попытке смоделировать процессы головного мозга.\n",
        "Сегодня мы стоим на пороге новой эры, когда модели могут генерировать связный текст, решать сложные задачи и даже проходить экзамены лучше среднестатистического студента.\n",
        "При этом базовые принципы остались те же: сеть получает входные данные, преобразует их через множество слоёв с весами, применяет нелинейные активации и постепенно учится предсказывать правильный ответ, минимизируя ошибку.\n",
        "В нашей задаче посимвольного предсказания мы видим эти принципы в самом чистом виде: каждая буква, пробел или знак препинания превращается в вектор, подаётся в рекуррентную ячейку, а сеть должна угадать, какой символ идёт дальше.\n",
        "Казалось бы, примитивно — но даже на таком простом примере прекрасно видно, насколько сильно лстм сети превосходит классические сети Элмана и Джордана по способности запоминать долгосрочные зависимости в последовательности.\n",
        "Нейронные сети представляют собой особый подход к созданию искусственного интеллекта, основанный на принципах работы человеческого мозга. Эти системы состоят из многочисленных взаимосвязанных узлов, которые можно сравнить с нервными клетками. Каждый такой узел получает сведения, обрабатывает их и передает дальше, в результате чего сеть постепенно учится распознавать закономерности и принимать решения.\n",
        "Обучение нейронной сети происходит путём многократного анализа примеров. Сначала система получает исходные данные и делает предположение о правильном ответе. Затем специальный алгоритм сравнивает полученный результат с эталоном и вычисляет величину ошибки. После этого связи между узлами немного изменяются таким образом, чтобы в следующий раз ошибка была меньше. Постепенное повторение этого процесса приводит к тому, что сеть всё точнее выполняет поставленную задачу.\n",
        "Существует несколько типов нейронных сетей, и каждая из них подходит для определённых целей. Самые простые используются для анализа числовых данных и выполнения сравнительно несложных вычислений. Более сложные разновидности способны распознавать изображения, выделять на них объекты, понимать устную речь, переводить текст с одного языка на другой или анализировать временные последовательности, например изменения температуры, трафика или финансовых показателей. Особые виды нейронных сетей умеют создавать новые изображения, тексты и даже музыку, опираясь на изученные примеры.\n",
        "Одной из главных причин популярности нейронных сетей является их способность самостоятельно находить сложные зависимости в больших объёмах данных. В отличие от традиционных программ, где человек заранее определяет правила, нейронная сеть извлекает необходимые закономерности сама. Это делает её особенно ценной в тех областях, где невозможно заранее описать все условия, например в медицине, где анализируются снимки и показания пациентов, или в сфере транспорта, где необходимо распознавать дорожную ситуацию.\n",
        "При всех своих достоинствах нейронные сети имеют и недостатки. Им требуется много данных для обучения, причём эти данные должны быть достаточно качественными. Кроме того, работа таких систем нередко требует мощного оборудования. Ещё одной проблемой является то, что нейронная сеть действует как «чёрный ящик»: она даёт результат, но определить, почему именно он получился, бывает трудно.\n",
        "Тем не менее развитие нейронных сетей продолжается очень быстро. Учёные стремятся сделать их более понятными, менее требовательными к ресурсам и способными обучаться на меньшем количестве примеров. Уже сегодня нейронные сети применяются в промышленности, медицине, науке, образовании, быту и развлечениях, и в будущем их роль будет только расти. Благодаря способности анализировать огромные массивы сведений и адаптироваться под новые задачи нейронные сети становятся одним из важнейших инструментов технологий будущего.\n",
        "Нейронные сети всё чаще становятся основой современных технологий, поскольку позволяют решать задачи, которые раньше казались чрезвычайно сложными или вовсе невозможными. Их можно представить как множество взаимосвязанных элементов, которые обмениваются сигналами и постепенно учатся понимать окружающий мир. Такой подход напоминает работу человеческого мозга, где миллиарды клеток взаимодействуют друг с другом, формируя мысли, образы и решения.\n",
        "Процесс обучения нейронной сети похож на обучение человека. Сначала она делает множество ошибок, неправильно реагирует на входные данные, но с каждым новым примером исправляется и становится точнее. Огромное преимущество таких систем состоит в том, что они способны учитывать множество факторов одновременно, замечать тонкие скрытые признаки и делать выводы, недоступные обычным алгоритмам.\n",
        "Благодаря своим возможностям нейронные сети применяются в самых разных областях. В медицине они помогают врачам обнаруживать заболевания на ранней стадии, анализируя снимки и результаты обследований. В промышленности они используются для контроля качества продукции и прогнозирования возможных неисправностей оборудования. В сельском хозяйстве — для оценки состояния почвы, растений и планирования урожайности. В быту — в умных устройствах, которые распознают голос человека, управляют техникой, помогают выбирать музыку или фильмы.\n",
        "Особенно впечатляет способность нейронных сетей создавать что-то новое. Они могут составлять тексты, рисовать картины, придумывать мелодии и даже имитировать почерк человека. Это открывает огромные возможности для творчества, образования и развлечений. При этом созданные сети продолжают развиваться, становясь всё более сложными и мощными.\n",
        "Однако важно помнить, что нейронные сети требуют осторожного обращения. Поскольку они учатся на данных, то качество их работы полностью зависит от того, какие сведения они получают. Если данные искажены или неполны, сеть может сделать неправильные выводы. Поэтому разработчики тщательно подбирают информацию для обучения и контролируют процесс, чтобы результаты были максимально надёжными.\n",
        "В будущем нейронные сети станут ещё более тесно связаны с повседневной жизнью. Они будут помогать принимать решения, работать, учиться и творить. Скорее всего, они станут такой же привычной частью жизни, как электричество или интернет. И хотя они не заменят человека, их возможности станут важным дополнением к человеческому разуму, расширяя горизонты науки, техники и искусства.\n",
        "Современные нейронные сети становятся неотъемлемой частью научного прогресса. Их главное достоинство заключается в способности самостоятельно извлекать полезную информацию из больших массивов данных. Когда человеку нужно проанализировать огромное количество сведений, он может легко упустить важные детали. Нейронная сеть, напротив, способна обработать данные во всём их объёме и обнаружить скрытые закономерности, которые не видны невооружённым глазом.\n",
        "Процесс работы нейронной сети основан на передаче сигналов между множеством связанных элементов. Каждый элемент получает данные, преобразует их и передаёт другим элементам. Сеть постепенно формирует внутренние связи, которые позволяют ей распознавать образы, понимать структуру информации и принимать решения. Благодаря этому нейронные сети успешно справляются с задачами, где требуется не просто вычисление, а именно понимание.\n",
        "Одним из интересных свойств нейронных сетей является их способность улучшать свои результаты со временем. Чем больше примеров они анализируют, тем увереннее становятся их выводы. Таким образом, система постоянно совершенствуется, что позволяет применять её в меняющихся условиях. Например, если сеть отвечает за наблюдение за дорожной обстановкой, то с каждым днём она лучше различает машины, пешеходов и препятствия, делая управление транспортом более безопасным.\n",
        "Нейронные сети уже нашли применение в искусстве, науке, промышленности и обучении. В художественной сфере они помогают создавать новые изображения и стили, а также вдохновляют творцов на необычные идеи. В научных исследованиях они ускоряют анализ сложных процессов, моделирование природных явлений и поиск закономерностей. В промышленности они позволяют предсказывать поломки техники, оптимизировать производство и снижать затраты. В обучении нейронные сети используются для создания индивидуальных программ, которые учитывают способности и темп каждого ученика.\n",
        "Несмотря на все достижения, нейронные сети всё ещё находятся в стадии активного развития. Учёные продолжают искать новые способы ускорить обучение, уменьшить потребность в огромных вычислительных ресурсах и сделать работу таких систем понятнее для человека. Одной из главных задач является создание нейронных сетей, способных объяснять свои решения. Это особенно важно там, где цена ошибки слишком высока — например, в медицине или транспортной безопасности.\n",
        "В будущем нейронные сети, вероятно, станут ещё более универсальными помощниками. Они смогут не только анализировать информацию, но и взаимодействовать с человеком almost на равных, понимать контекст, настроение, особенности мышления. Такой шаг позволит создать технику, которая не просто выполняет команды, а действительно понимает пользователя.\n",
        "\"\"\".strip() * 2\n",
        "\n",
        "text = text[:]\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "\n",
        "data = np.array([char2idx[ch] for ch in text], dtype=np.int32)\n",
        "X_all, Y_all = data[:-1], data[1:]\n",
        "split = int(0.9 * len(X_all))\n",
        "X_train, Y_train = X_all[:split], Y_all[:split]\n",
        "X_test, Y_test = X_all[split:], Y_all[split:]\n",
        "print(f\"len text={len(text)}, vocab={vocab_size}, train={len(X_train)}, test={len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, random\n",
        "np.random.seed(42); random.seed(42)\n",
        "\n",
        "# =========================\n",
        "# 1) Подготовка данных\n",
        "# =========================\n",
        "text = \"\"\"\n",
        "Нейронные — это мощный инструмент, который способен находить закономерности в данных.\n",
        "Они применяются в обработке языка, изображений, в прогнозировании и многом другом.\n",
        "В этой небольшой демонстрации мы обучаем простые рекуррентные сети для предсказания следующего символа.\n",
        "Искусственный интеллект, нейронные сети, машинное обучение — что на самом деле означают все эти нынче популярные понятия?\n",
        "Для большинства непосвященных людей, коим являюсь и я сам, они всегда казались чем-то фантастическим.\n",
        "Впервые понятие искусственных нейронных сетей (ИНС) возникло при попытке смоделировать процессы головного мозга.\n",
        "Сегодня мы стоим на пороге новой эры, когда модели могут генерировать связный текст, решать сложные задачи и даже проходить экзамены лучше среднестатистического студента.\n",
        "При этом базовые принципы остались те же: сеть получает входные данные, преобразует их через множество слоёв с весами, применяет нелинейные активации и постепенно учится предсказывать правильный ответ, минимизируя ошибку.\n",
        "В нашей задаче посимвольного предсказания мы видим эти принципы в самом чистом виде: каждая буква, пробел или знак препинания превращается в вектор, подаётся в рекуррентную ячейку, а сеть должна угадать, какой символ идёт дальше.\n",
        "Казалось бы, примитивно — но даже на таком простом примере прекрасно видно, насколько сильно лстм сети превосходит классические сети Элмана и Джордана по способности запоминать долгосрочные зависимости в последовательности.\n",
        "\"\"\".strip() * 3\n",
        "\n",
        "text = text[:]\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "\n",
        "data = np.array([char2idx[ch] for ch in text], dtype=np.int32)\n",
        "X_all, Y_all = data[:-1], data[1:]\n",
        "split = int(0.9 * len(X_all))\n",
        "X_train, Y_train = X_all[:split], Y_all[:split]\n",
        "X_test, Y_test = X_all[split:], Y_all[split:]\n",
        "print(f\"len text={len(text)}, vocab={vocab_size}, train={len(X_train)}, test={len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yljPc31N_9FS",
        "outputId": "4c5c2713-0b24-47a2-e9f5-3e63fdb1438e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len text=4368, vocab=51, train=3930, test=437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2) Вспомогательные функции\n",
        "# =========================\n",
        "def onehot(index, vocab_size):\n",
        "    v = np.zeros(vocab_size, dtype=np.float32)\n",
        "    v[index] = 1.0\n",
        "    return v\n",
        "\n",
        "def to_onehot_seq(indices, vocab_size):\n",
        "    return [onehot(int(i), vocab_size) for i in indices]\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e)\n",
        "\n",
        "def cross_entropy_from_probs(probs, target_idx):\n",
        "    return -np.log(probs[target_idx] + 1e-12)\n",
        "\n",
        "def sample_with_temperature(p, T=1.0):\n",
        "    p = np.asarray(p, dtype=np.float64)\n",
        "    p = np.log(p + 1e-12) / T\n",
        "    p = np.exp(p - np.max(p))\n",
        "    p = p / np.sum(p)\n",
        "    return np.random.choice(len(p), p=p)"
      ],
      "metadata": {
        "id": "EWxnOfe1AAF9"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) Elman RNN\n",
        "# =========================\n",
        "class ElmanRNN:\n",
        "    def __init__(self, vocab_size, hidden_size=128):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        limit = np.sqrt(6.0 / (vocab_size + hidden_size))\n",
        "        self.Wxh = np.random.uniform(-limit, limit, (hidden_size, vocab_size)).astype(np.float32)\n",
        "        self.Whh = np.random.uniform(-limit, limit, (hidden_size, hidden_size)).astype(np.float32)\n",
        "        self.Why = np.random.uniform(-limit, limit, (vocab_size, hidden_size)).astype(np.float32)\n",
        "        self.bh = np.zeros(hidden_size, dtype=np.float32)\n",
        "        self.by = np.zeros(vocab_size, dtype=np.float32)\n",
        "\n",
        "    def forward(self, inputs, h0=None):\n",
        "        T = len(inputs)\n",
        "        if h0 is None:\n",
        "            h_prev = np.zeros(self.hidden_size, dtype=np.float32)\n",
        "        else:\n",
        "            h_prev = h0.copy()\n",
        "        hs = np.zeros((T + 1, self.hidden_size), dtype=np.float32)\n",
        "        hs[0] = h_prev\n",
        "        ps = np.zeros((T, self.vocab_size), dtype=np.float32)\n",
        "        for t in range(T):\n",
        "            x = inputs[t]\n",
        "            hs[t + 1] = np.tanh(self.Wxh @ x + self.Whh @ hs[t] + self.bh)\n",
        "            o = self.Why @ hs[t + 1] + self.by\n",
        "            ps[t] = softmax(o)\n",
        "        return hs, ps\n",
        "\n",
        "    def bptt(self, inputs, targets, lr=0.005, clip=5.0, h0=None):\n",
        "        T = len(inputs)\n",
        "        hs, ps = self.forward(inputs, h0)\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dWhy = np.zeros_like(self.Why)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dby = np.zeros_like(self.by)\n",
        "        loss = 0.0\n",
        "        dh_next = np.zeros(self.hidden_size, dtype=np.float32)\n",
        "\n",
        "        for t in range(T):\n",
        "            loss += cross_entropy_from_probs(ps[t], int(targets[t]))\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            dy = ps[t].copy()\n",
        "            dy[int(targets[t])] -= 1.0\n",
        "            dWhy += np.outer(dy, hs[t + 1])\n",
        "            dby += dy\n",
        "            dh = self.Why.T @ dy + dh_next\n",
        "            dh_raw = (1.0 - hs[t + 1] ** 2) * dh\n",
        "            dbh += dh_raw\n",
        "            dWxh += np.outer(dh_raw, inputs[t])\n",
        "            dWhh += np.outer(dh_raw, hs[t])\n",
        "            dh_next = self.Whh.T @ dh_raw\n",
        "\n",
        "        for g in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(g, -clip, clip, out=g)\n",
        "\n",
        "        self.Wxh -= lr * dWxh\n",
        "        self.Whh -= lr * dWhh\n",
        "        self.Why -= lr * dWhy\n",
        "        self.bh -= lr * dbh\n",
        "        self.by -= lr * dby\n",
        "\n",
        "        return loss / T, hs[-1].copy()\n",
        "\n",
        "    def predict_next(self, x_onehot, h_prev):\n",
        "        h = np.tanh(self.Wxh @ x_onehot + self.Whh @ h_prev + self.bh)\n",
        "        o = self.Why @ h + self.by\n",
        "        p = softmax(o)\n",
        "        return p, h"
      ],
      "metadata": {
        "id": "nCqu0gpoAFjd"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4) Jordan RNN\n",
        "# =========================\n",
        "class JordanRNN:\n",
        "    def __init__(self, vocab_size, hidden_size=128):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        limit = np.sqrt(6.0 / (vocab_size + hidden_size))\n",
        "        self.Wxh = np.random.uniform(-limit, limit, (hidden_size, vocab_size)).astype(np.float32)\n",
        "        self.Wch = np.random.uniform(-limit, limit, (hidden_size, vocab_size)).astype(np.float32)\n",
        "        self.Why = np.random.uniform(-limit, limit, (vocab_size, hidden_size)).astype(np.float32)\n",
        "        self.bh = np.zeros(hidden_size, dtype=np.float32)\n",
        "        self.by = np.zeros(vocab_size, dtype=np.float32)\n",
        "\n",
        "    def forward(self, inputs, ctx0=None):\n",
        "        T = len(inputs)\n",
        "        if ctx0 is None:\n",
        "            ctx_prev = np.zeros(self.vocab_size, dtype=np.float32)\n",
        "        else:\n",
        "            ctx_prev = ctx0.copy()\n",
        "        hs = np.zeros((T + 1, self.hidden_size), dtype=np.float32)\n",
        "        ps = np.zeros((T, self.vocab_size), dtype=np.float32)\n",
        "        for t in range(T):\n",
        "            x = inputs[t]\n",
        "            hs[t + 1] = np.tanh(self.Wxh @ x + self.Wch @ ctx_prev + self.bh)\n",
        "            o = self.Why @ hs[t + 1] + self.by\n",
        "            p = softmax(o)\n",
        "            ps[t] = p\n",
        "            ctx_prev = p\n",
        "        return hs, ps\n",
        "\n",
        "    def bptt(self, inputs, targets, lr=0.01, clip=5.0, ctx0=None):\n",
        "        T = len(inputs)\n",
        "        hs, ps = self.forward(inputs, ctx0)\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWch = np.zeros_like(self.Wch)\n",
        "        dWhy = np.zeros_like(self.Why)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dby = np.zeros_like(self.by)\n",
        "        loss = 0.0\n",
        "\n",
        "        for t in range(T):\n",
        "            loss += cross_entropy_from_probs(ps[t], int(targets[t]))\n",
        "\n",
        "        for t in reversed(range(T)):\n",
        "            dy = ps[t].copy()\n",
        "            dy[int(targets[t])] -= 1.0\n",
        "            dWhy += np.outer(dy, hs[t + 1])\n",
        "            dby += dy\n",
        "            dh = self.Why.T @ dy\n",
        "            dh_raw = (1.0 - hs[t + 1] ** 2) * dh\n",
        "            dbh += dh_raw\n",
        "            dWxh += np.outer(dh_raw, inputs[t])\n",
        "            if t > 0:\n",
        "                dWch += np.outer(dh_raw, ps[t - 1])\n",
        "        for g in [dWxh, dWch, dWhy, dbh, dby]:\n",
        "            np.clip(g, -clip, clip, out=g)\n",
        "\n",
        "        self.Wxh -= lr * dWxh\n",
        "        self.Wch -= lr * dWch\n",
        "        self.Why -= lr * dWhy\n",
        "        self.bh -= lr * dbh\n",
        "        self.by -= lr * dby\n",
        "\n",
        "        return loss / T, ps[-1].copy()\n",
        "\n",
        "    def predict_next(self, x_onehot, ctx_prev):\n",
        "        h = np.tanh(self.Wxh @ x_onehot + self.Wch @ ctx_prev + self.bh)\n",
        "        o = self.Why @ h + self.by\n",
        "        p = softmax(o)\n",
        "        return p, h, p"
      ],
      "metadata": {
        "id": "TFMi4MLLAQ0v"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5) Обучение\n",
        "# =========================\n",
        "def train_model(model, X, Y, seq_len=50, epochs=100, lr=0.005, name=\"Model\"):\n",
        "    n = len(X)\n",
        "    base_lr = lr\n",
        "\n",
        "    # начальные состояния\n",
        "    if isinstance(model, ElmanRNN):\n",
        "        state = np.zeros(model.hidden_size, dtype=np.float32)\n",
        "    else:\n",
        "        state = np.zeros(model.vocab_size, dtype=np.float32)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        lr = base_lr * (0.97 ** (epoch // 5))\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "\n",
        "        for i in range(0, n - seq_len, seq_len):\n",
        "            X_seq = to_onehot_seq(X[i:i + seq_len], model.vocab_size)\n",
        "            Y_seq = Y[i:i + seq_len]\n",
        "\n",
        "            if isinstance(model, ElmanRNN):\n",
        "                loss, state = model.bptt(X_seq, Y_seq, lr=lr, clip=1.0, h0=state)\n",
        "                state = np.clip(state, -5, 5)\n",
        "            else:\n",
        "                loss, state = model.bptt(X_seq, Y_seq, lr=lr, clip=1.0, ctx0=state)\n",
        "                state = np.clip(state, 0, 1)\n",
        "\n",
        "            total_loss += loss * len(X_seq)\n",
        "            count += len(X_seq)\n",
        "\n",
        "        avg_loss = total_loss / count\n",
        "        loss_history.append(avg_loss)\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
        "            print(f\"[{name}] epoch {epoch}/{epochs}, avg loss={avg_loss:.4f}, lr={lr:.5f}\")\n",
        "\n",
        "    return loss_history"
      ],
      "metadata": {
        "id": "GMSqUw2JATkb"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 6) Accuracy и генерация\n",
        "# =========================\n",
        "def compute_accuracy(model, X, Y):\n",
        "    correct, total = 0, 0\n",
        "    if isinstance(model, ElmanRNN):\n",
        "        h = np.zeros(model.hidden_size, dtype=np.float32)\n",
        "        for i in range(len(X)):\n",
        "            x = onehot(int(X[i]), model.vocab_size)\n",
        "            p, h = model.predict_next(x, h)\n",
        "            if np.argmax(p) == int(Y[i]):\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    else:\n",
        "        ctx = np.zeros(model.vocab_size, dtype=np.float32)\n",
        "        for i in range(len(X)):\n",
        "            x = onehot(int(X[i]), model.vocab_size)\n",
        "            p, h, ctx = model.predict_next(x, ctx)\n",
        "            if np.argmax(p) == int(Y[i]):\n",
        "                correct += 1\n",
        "            total += 1\n",
        "    return correct / total\n",
        "\n",
        "def generate(model, seed, length=200, T=0.8):\n",
        "    out = seed\n",
        "    if isinstance(model, ElmanRNN):\n",
        "        h = np.zeros(model.hidden_size, dtype=np.float32)\n",
        "        for ch in seed[:-1]:\n",
        "            x = onehot(char2idx.get(ch, 0), model.vocab_size)\n",
        "            _, h = model.predict_next(x, h)\n",
        "        last_idx = char2idx.get(seed[-1], 0)\n",
        "        for _ in range(length):\n",
        "            x = onehot(last_idx, model.vocab_size)\n",
        "            p, h = model.predict_next(x, h)\n",
        "            next_idx = sample_with_temperature(p, T)\n",
        "            out += idx2char[next_idx]\n",
        "            last_idx = next_idx\n",
        "    else:\n",
        "        ctx = np.zeros(model.vocab_size, dtype=np.float32)\n",
        "        for ch in seed[:-1]:\n",
        "            x = onehot(char2idx.get(ch, 0), model.vocab_size)\n",
        "            p, h, ctx = model.predict_next(x, ctx)\n",
        "        last_idx = char2idx.get(seed[-1], 0)\n",
        "        for _ in range(length):\n",
        "            x = onehot(last_idx, model.vocab_size)\n",
        "            p, h, ctx = model.predict_next(x, ctx)\n",
        "            next_idx = sample_with_temperature(p, T)\n",
        "            out += idx2char[next_idx]\n",
        "            last_idx = next_idx\n",
        "    return out"
      ],
      "metadata": {
        "id": "BAaiC_QBAWm-"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 7) Запуск обучения и генерации\n",
        "# =========================\n",
        "elman = ElmanRNN(vocab_size, hidden_size=128)\n",
        "jordan = JordanRNN(vocab_size, hidden_size=128)\n",
        "\n",
        "print(\"\\nTraining ElmanRNN...\")\n",
        "train_model(elman, X_train, Y_train, seq_len=80, epochs=10, lr=0.005, name=\"Elman\")\n",
        "\n",
        "print(\"\\nTraining JordanRNN...\")\n",
        "train_model(jordan, X_train, Y_train, seq_len=80, epochs=10, lr=0.01, name=\"Jordan\")\n",
        "\n",
        "acc_e = compute_accuracy(elman, X_test, Y_test)\n",
        "acc_j = compute_accuracy(jordan, X_test, Y_test)\n",
        "print(f\"\\nAccuracy — Elman: {acc_e*100:.2f}%, Jordan: {acc_j*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69n0JJnAZMn",
        "outputId": "b51ad5d5-2422-4bd0-95dc-a2106554124c"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training ElmanRNN...\n",
            "[Elman] epoch 1/10, avg loss=3.0766, lr=0.00500\n",
            "[Elman] epoch 5/10, avg loss=2.1646, lr=0.00485\n",
            "[Elman] epoch 10/10, avg loss=1.8683, lr=0.00470\n",
            "\n",
            "Training JordanRNN...\n",
            "[Jordan] epoch 1/10, avg loss=2.8811, lr=0.01000\n",
            "[Jordan] epoch 5/10, avg loss=2.3928, lr=0.00970\n",
            "[Jordan] epoch 10/10, avg loss=2.3204, lr=0.00941\n",
            "\n",
            "Accuracy — Elman: 39.40%, Jordan: 30.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 8) ГОТОВАЯ LSTM ИЗ PyTorch\n",
        "# =========================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class PyTorchLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=128, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.input_proj = nn.Linear(vocab_size, vocab_size, bias=False)\n",
        "        self.lstm = nn.LSTM(vocab_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.output_proj = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        x_onehot = torch.zeros(x.size(0), x.size(1), vocab_size, device=x.device)\n",
        "        x_onehot.scatter_(2, x.unsqueeze(-1), 1.0) # настоящие one-hot тензоры\n",
        "\n",
        "        x_emb = self.input_proj(x_onehot)\n",
        "        out, hidden = self.lstm(x_emb, hidden)\n",
        "        out = self.output_proj(out)\n",
        "        return out, hidden\n",
        "\n",
        "# Данные (уже есть выше)\n",
        "X_train_t = torch.from_numpy(X_train).long().to(device)\n",
        "Y_train_t = torch.from_numpy(Y_train).long().to(device)\n",
        "X_test_t  = torch.from_numpy(X_test).long().to(device)\n",
        "Y_test_t  = torch.from_numpy(Y_test).long().to(device)\n",
        "\n",
        "torch_lstm = PyTorchLSTM(vocab_size, hidden_size=128, num_layers=1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(torch_lstm.parameters(), lr=0.005)\n",
        "\n",
        "print(\"\\nTraining PyTorch LSTM\")\n",
        "seq_len = 80\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    torch_lstm.train()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    i = 0\n",
        "    while i + seq_len < len(X_train_t):\n",
        "        seq = X_train_t[i:i + seq_len]\n",
        "        target = Y_train_t[i:i + seq_len]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = torch_lstm(seq)\n",
        "        loss = criterion(output.view(-1, vocab_size), target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(torch_lstm.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * seq_len\n",
        "        count += seq_len\n",
        "        i += seq_len\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1 or epoch == epochs:\n",
        "        print(f\"[PyTorch LSTM] epoch {epoch}/{epochs}, avg loss={total_loss/count:.4f}\")\n",
        "\n",
        "# --------------------- Accuracy ---------------------\n",
        "def compute_accuracy_torch(model, X, Y):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    hidden = None\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(X)):\n",
        "            x = X[i].unsqueeze(0)\n",
        "            y = Y[i]\n",
        "            output, hidden = model(x, hidden)\n",
        "            pred = output[0, -1].argmax().item()\n",
        "            if pred == y.item():\n",
        "                correct += 1\n",
        "    return correct / len(X)\n",
        "\n",
        "acc_pt = compute_accuracy_torch(torch_lstm, X_test_t, Y_test_t)\n",
        "\n",
        "# --------------------- Генерация ---------------------\n",
        "def generate_torch(model, seed_text, length=600, temperature=0.7):\n",
        "    model.eval()\n",
        "    generated = seed_text\n",
        "    hidden = None\n",
        "\n",
        "    # Прогрев сети по символам из seed\n",
        "    for ch in seed_text:\n",
        "        idx = torch.tensor([[char2idx[ch]]], device=device)\n",
        "        _, hidden = model(idx, hidden)\n",
        "\n",
        "    cur_idx = char2idx[seed_text[-1]]\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            x = torch.tensor([[cur_idx]], device=device)\n",
        "            output, hidden = model(x, hidden)\n",
        "            logits = output[0, -1] / temperature\n",
        "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "            cur_idx = np.random.choice(vocab_size, p=probs)\n",
        "            generated += idx2char[cur_idx]\n",
        "    return generated\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-spAtMbsJa0N",
        "outputId": "6f920f2b-c974-4b70-e856-e09c514057d2"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Training PyTorch LSTM\n",
            "[PyTorch LSTM] epoch 1/50, avg loss=2.6912\n",
            "[PyTorch LSTM] epoch 5/50, avg loss=1.3474\n",
            "[PyTorch LSTM] epoch 10/50, avg loss=0.7714\n",
            "[PyTorch LSTM] epoch 15/50, avg loss=0.5095\n",
            "[PyTorch LSTM] epoch 20/50, avg loss=0.3916\n",
            "[PyTorch LSTM] epoch 25/50, avg loss=0.3322\n",
            "[PyTorch LSTM] epoch 30/50, avg loss=0.3049\n",
            "[PyTorch LSTM] epoch 35/50, avg loss=0.2992\n",
            "[PyTorch LSTM] epoch 40/50, avg loss=0.2799\n",
            "[PyTorch LSTM] epoch 45/50, avg loss=0.2797\n",
            "[PyTorch LSTM] epoch 50/50, avg loss=0.2863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"нейронные\"\n",
        "print(\"\\nGenerated (Elman):\\n\", generate(elman, seed, 200, 0.7))\n",
        "print(\"\\nGenerated (Jordan):\\n\", generate(jordan, seed, 200, 0.5))\n",
        "print(\"\\nLSTM ИЗ PyTorch:\\n\", generate_torch(torch_lstm, seed, length=200, temperature=0.5))\n",
        "print(f\"\\nAccuracy — Elman: {acc_e*100:.2f}%, Jordan: {acc_j*100:.2f}%, PyTorch LSTM: {acc_pt*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbPINTnTAt4q",
        "outputId": "9146e975-edac-40e8-b687-fb2716d5f7a0"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated (Elman):\n",
            " нейронные сети иременити пленолечеть москаже ипродрамые тствий эттвот могутряцтости метьтаеннос дотва, ратилыможее позвравьт. Влограмодия татсовта.тНо чостья. Бание симним. Сатрутвани стрранати програчедани по\n",
            "\n",
            "Generated (Jordan):\n",
            " нейронные ре пронные и коружениме немей и и нелать недеть ствать сет ваме при имерать и прожетвамедирачетуе резнейраножные эти стать понные вожные прованыени стани сет важные сет и чемнымининистображдать обрат\n",
            "\n",
            "LSTM ИЗ PyTorch:\n",
            " нейронные должны учатся невозмодировательности.\n",
            "Нейронные сети применяются в самых разных областях. В медицине они помогают состоинство заключается в самы нипринципы в самом чистом более безоправляют техникой,\n",
            "\n",
            "Accuracy — Elman: 40.03%, Jordan: 30.17%, PyTorch LSTM: 79.80%\n"
          ]
        }
      ]
    }
  ]
}